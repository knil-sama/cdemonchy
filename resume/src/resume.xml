<personal>
<firstname>Clement</firstname><lastname>Demonchy</lastname>
<title>Senior data engineer and data architect freelance</title>
<profile_pic>images/profile.jpg</profile_pic>
</personal>
<online>
<website>cdemonchy.com</website>
<github_pseudo>_knil-sama</github_pseudo>
<linkedin>clement-demonchy-55327778</linkedin>
</online>
<resume>
    <about_me> 
    I have more than 10 years of experience working as a data engineer mainly in Python & AWS, assuming devops, architect and lead roles when needed.<br/>
    I prefer working on missions where I can focus on workflows that bring value while making sure that the tradeoff (cost, reliability, performance) are understood and managed.<br/>
    I appreciate when I can interact with the infra and product team directly.<br/>
    I am most proficient at: <mark>AWS</mark>, <mark>Python</mark>, <mark>PostgreSQL</mark>, <mark>Kubernetes</mark> and <mark>Databricks<mark>
    I have experiences on <mark>Azure</mark> and <mark>GCP</mark> as well as <mark>Scala</mark>, <mark>Neo4j</mark> and <mark>MongoDB</mark>
    I like to dabble in <mark>Rust</mark> and <mark>TLA</mark>
    </about_me>
    <projects>
        <project>
            <title>Blog</title>
            <website>
                <url>https://cdemonchy.com/blog/</url>
                <link_text>Blog</link_text>
            </website>
            <description>Technical articles I wrote</description>
        </project>
        <project>
            <title>Boite postale</title>
            <website>
                <url>https://boitepostale.cdemonchy.com/</url>
                <link_text>Demo</link_text>
            </website>
            <github>
                <url>https://github.com/knil-sama/boite_postale</url>
                <link_text>repository Boite postale</link_text>
            </github>
            <description>Fetch open data from french government using <mark>python</mark> to deploy an interactive map in <mark>Folium</mark> on <mark>AWS</mark> with <mark>Github Actions</mark></description>
        </project>
        <project>
            <title>Rubbersearch</title>
            <github>
                <url>https://github.com/knil-sama/rubbersearch</url>
                <link_text>repository Rubbersearch</link_text>
            </github>
            <description>Rust code that processes a local file to expose data using a REST API</description>
        </project>
        <project>
            <title>Rubbersearch</title>
            <github>
                <url>https://github.com/knil-sama/rubbersearch</url>
                <link_text>repository Rubbersearch</link_text>
            </github>
            <description>Rust code that processes a local file to expose data using a REST API</description>
        </project>
        <project>
            <title>ITM</title>
            <github>
                <url>https://github.com/knil-sama/itm</url>
                <link_text>repository for ITM</link_text>
            </github>
            <description>Shows how to run an <mark>airflow</mark> workflow  in <mark>Python</mark>, distibuted with <mark>Flower & <mark>Celery</mark> to process images, extracting metadata before storing it in a <mark>MongoDB</mark> database,<br/>
            there are some failures that will randomly appear when running the workflow to show how airflow handles it 
            </description>
        </project>
    <projects>
    <experiences>
        <experience>
            <company>
                <name>Doctolib</name>
                <url>https:wwww.doctolib.fr</url>
                <description>Doctolib is an appointment solution for healthcare professional and patient</description>
            </company>
            <start_date></start_date>
            <end_date></end_date>
            <role>Data Engineer</role>
            <description>In a team responsible for exposition and anonymization of production tables
            -Change Data Capture of relational database to iceberg table using Debezium
            -Create mirror tables with Pyspark job on EMR serverless
            -Create and maintain dataset on google cloud using Crossplane and Kopf
            -Install resources on GKE using Argocd
            </description>
        </experience>
        <experience>      
            <company>
                <name>Be:mo (at TotalEnergies)</name>
                <url>http://www.be-mo.io</url>
                <description>Be:mo provide a B2B solutions to help recharge electric vehicule (truck and car)</description>
            </company>
            <role>Data Engineer</role>
            <start_date></start_date>
            <end_date></end_date>
            <description>In a team responsible for working with electric vehicle supply equipment (EVSE) and exposing 
            it with REST API for multiple BtB companies
            -Migration of data workflow from Azure synapse to AWS databricks
            -Writing CI job using Github Actions and Databricks bundle
            -Using unity catalog to provide external access to dataset for services like PowerBi
            - Usage of DLT table for streaming dataset
            -Setting test for spark on dev machines to improve feedback
            -Managing AWS & Databricks resources using terraform (tagging, creation, deletion)
            -Define alerts based on Lambda metrics using cloudwatch
            -Expose metrics of data workflow with PostgreSQL Aurora and grafana
            -Documentation on workflow using Mermaid on Github
            -Analyzing and reducing cost on Azure and AWS, for example by  dividing cost by 5 for AWS Lambda     
            exploiting caching between service and improving batching of SQS elements
            -Optimize geolocalization operation on PostgreSQL and detect invalid coordinates
            </description>
        <experience>
            <company>
                <name>Skillup</name>
                <url>https:wwww.skillup.co</url>
                <description>Skillup is a platform to make HR training management easy including interviews and skills </description>
            </company>
            <start_date></start_date>
            <end_date></end_date>
            <role>Lead data Engineer</role>
            <description>          I was responsible for a 4 person team of 3 Data engineers and 1 ML engineer at Skillup, an SaaS helping RH in the company to provide a better experience providing training for their employees and evaluating impact.<br/>
            The team had a broad scope covering 4 principle subjects<br/>
            -External data flux from customer with client import (user, booked training, …)  & export (training evaluation)<br/>
            -Scrapping & cleaning training website for our catalog<br/>
            -Provide internal metrics & dashboard so products team can gain insights<br/>
            -Use latest AI technologies (LLM) to implement new feature for customer<br/>
            My first task when starting here was to put in place a CI/CD using github action to structure coding practice with automated testing and linting.<br/>
            All the while implementing team's rituals (postmortem, retrospective, daily, ownership rotation) as well as mentoring & pairing to share best coding practice (ruff, pydantic, pytest, …).<br/>
            This allowed  us to refactor existing code more easily and made the team more resilient with people being able to work on other team member code.<br/>
            The scrapping project required migration of Airflow orchestrated data pipelines that use Zyte for scrapping online training websites and extracting data using NLP models.<br/>
            The frontend of Skillup being developed in Typescript we rely on GRPC API to interact with their system.<br/>
            New services developed to allow complex importation of customer data were an Fastapi REST API to query Neo4j, MongoDB and PostgreSQL databases to reconcile existing data.<br/>
            We then provided a website in intern developed in Dash for the product team in case a manual validation was required.<br/>
            For the dashboarding we wanted to give a lot of autonomy for no tech savvy user and deployed both Metabase and Superset.<br/>
            For the IA related project we needed to do some optimisation of OpenIA POC for generation of job's skills based on role title</description>
            </description>
        </experience>
        <experience>
            <company>
                <name>Talend</name>
                <url>https:wwww.talend.com</url>
                <description>Talend is an open source data integration platform. It provides various software and services for data integration, data management, enterprise application integration, data quality, cloud storage and Big Data.
                </description>
            </company>
            <start_date></start_date>
            <end_date></end_date>
            <role>Data Engineer</role>
            <description>
            I was the first data engineer of a growing team of data scientists (5 to 7 people) that worked exclusively on state of the art machine learning for customer and internal needs.<br/>
            We got a separate AWS account and a dedicated kubernetes namespace for our dev environment, as we were the first to do machine learning at Talend so a lot of new processes needed to be put in place.<br/>
            In this context the first months were used to document and set guidelines for coding, testing, packaging and deployment using Github action and Jenkins.<br/>
            We experimented with multiple platforms to deliver our solutions<br/>
            -Data pipeline on Databricks with Scala and Python (Pyspark & Koala) to feed ML prediction models<br/>
            -Deployment and optimisation of models to detect anomalies in customer’s jobs in python & Flask<br/>
            -Extraction using Talend studio job(Java) to dump postgresql database on s3 bucket as parquet<br/>
            -Extraction and computation on a Snowflake datawarehouse<br/>
            We finally settled on Sagemaker for most of our projects and my role included finops responsibilities<br/>
            -Managing AWS account resources using terraform (tagging, creation, deletion)<br/>
            -Define with team's lead budget alerts and reports by slack/email with related dashboards to provide context<br/>
            -Evaluation and optimisation regarding architecture of Sagemaker pipeline, model and endpoint on AWS<br/>
            Other responsibilities included making people more aware of security with presentation, training on secureflag platform and setting audit tools in the CI with veracode.
            </description>
        </experience>
        <experience>
            <company>
                <name>Jobteaser</name>
                <url>https:wwww.jobteaser.com</url>
                <description>JobTeaser is a French company that provides recruitment solutions to companies for the recruitment of young talents and a free career center software towards higher education institutions in Europe.
                </description>
            </company>
            <start_date></start_date>
            <end_date></end_date>
            <role>Data Engineer</role>
            <description>
            I joined Jobteaser as the second data engineer of a 5 person team that provided data analysis and machine learning for the whole company.<br/>
            The company was moving a startup to a scaleup so a lot of projects were doing migration from the legacy system to the new system.<br/>
            -Migrate Elastic Beanstalk REST API on AWS to Kubernetes.<br/>
            -Rewrite/deploy job classification REST API in Flask.<br/>
            -Write/deploy ETL and REST API for job recommendation project.<br/>
            Main project beside that was deploying the backbone of our data stack with a Kafka that allowed us to  stream our production database (MySQL) to our Postgresql using Debezium and change data capture patterns.<br/>
            We then loaded it into our datawarehouse (Redshift) that give data analyst opportunity to do realtime dashboard of backend activity
            </description>
        </experience>
        <experience>
            <company>
                <name>Ipsen</name>
                <url>https:wwww.ipsen.com</url>
                <description>Ipsen is a pharmaceutical company
               </description>
            </company>
            <start_date></start_date>
            <end_date></end_date>
            <role>Data Engineer</role>
            <description>
            Alongside another data engineer we exploring multiple projects to resolve issue encountered by marketing and research teams sharing progress during spring (3 weeks) for Ipsen an pharmaceutical company<br/>
            The two most advanced projects are listed below and all of them were hosted on AWS ec2 using docker-compose.<br/>
            -Team's responsible for handling clinical trials of new drugs needed insight on previous research done by other companies in the whole world, so we did data mining on official reports for specific diseases, cross referenced the sources then displayed it on an interactive map using Folium, we ended up pushing some fix to folium.<br/>
            -Marketing wanted to get feedback regarding new medicines on social media so we targeted tweets related to either the disease or drug,  developed an algorithm to identify key opinion leaders (either medical professional or public personality) then used nlp tools to do sentiment analysis.<br/>
            </description>
        </experience>
        <experience>
            <company>
                <name>Engie</name>
                <url>https:wwww.engie.com</url>
                <description>Engie is a French multinational electric utility company
                </description>
            </company>
            <start_date></start_date>
            <end_date></end_date>
            <role>Data Engineer</role>
            <description>
            At Engie I worked on 2 projects with them<br/>
            PAP:<br/>
            With another data engineer and an app team in order to provide a solution to traveling salesmen that need to plan their road, get customer information and sign contracts on their tablets.<br/>
            We had a huge focus on geolocation and we had to put in place<br/>
            -Mongodb to store customer's data (60 millions users)<br/>
            -REST API written in Java for the application team<br/>
            -Fix customer addresses using google maps data and apache Lucene<br/>
            This project was successfully deployed and is still in production.<br/>
            GDF 360:<br/>
            To help desk support of Engie to provide better service to customers we implemented a POC in 6 months that displayed all known previous interactions between a customer and the company (call, bills, contract, …) using a graph database (neo4j) on a temporal axis.<br/>
            </description>
        </experience>
    </experiences>
    <education>
        <title>Master's degree in Computer Science</title>
        <university>University Of Cergy-Pontoise (UCP)</university>
        <description>Main focus was on Java, Hadoop and SQL databases.</description>
        <start_date>2013</start_date>
        <end_date>2015</end_date>
    <education>
</resume>