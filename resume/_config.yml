baseurl: ""
# Site
repository: knil-sama/cdemonchy
# favicon: Directory of your favicon (eg. images/favicon.ico)(optional)

# Content configuration version
version: 2

# Personal info
name: Clement Demonchy
title: Freelance finops and data engineering

# Social links
github_username:  knil-sama
linkedin_username: clement-demonchy-55327778

# Dark Mode (true/false/never)
darkmode: false

# About Section
# about_title: About Me
about_profile_image: images/profile.jpg
about_content: | # this will include new lines to allow paragraphs
  I have 10 years of experience working as a data engineer in Python & AWS, assuming devops, architect and lead roles when needed.<br/>
  I prefer working on missions where I can focus on workflows that bring value while making sure that costs are understood and managed.<br/>
  I appreciate when I can interact with the infra and product team directly.<br/>
  I am most proficient at: <mark>AWS</mark>, <mark>Python</mark>, <mark>PostgreSQL</mark> and <mark>Kubernetes</mark>
  I like to dabble in <mark>Rust</mark> and <mark>TLA</mark>



content:
  - title: Projects # Title for the section
    layout: list # Type of content section (list/text)
    content:
      - layout: top-middle
        title: blog
        link: https://cdemonchy.com/blog/
        link_text: blog
        quote: >
          Technical articles I wrote
      - layout: top-middle
        title: Boite postale
        link: https://boitepostale.cdemonchy.com/
        link_text: Boite postale
        additional_links:
          - title: knil-sama/boite_postale
            icon: fab fa-github
            url: https://github.com/knil-sama/boite_postale
        quote: >
          Example of data visualization with Folium
        description: | # this
          Fetch open data from french government using <mark>python</mark> to deploy an interactive map on <mark>AWS</mark> with <mark>Travis CI</mark> 
      - layout: top-middle
        title: Rubbersearch
        additional_links:
          - title: knil-sama/rubbersearch
            icon: fab fa-github
            url: https://github.com/knil-sama/rubbersearch
        quote: >
          Rest api in Rust
        description: | # this
          Rust code that processes a local file to expose data using a REST API 
      - layout: top-middle
        title: ITM
        additional_links:
          - title: knil-sama/itm
            icon: fab fa-github
            url: https://github.com/knil-sama/itm
        quote: >
          Example of airflow workflow
        description: | # this
          Shows how to run an <mark>airflow</mark> workflow while processing images, extracting metadata before storing it in a mongodb database,<br/>
          there are some failures that will randomly appear when running the workflow to show how airflow handles it 
  - title: Experience
    layout: list
    content:
      - layout: left
        title: Skillup
        link: https:wwww.skillup.co
        sub_title: Lead data Engineer
        caption: 2022 - 2023
        quote: >
          Skillup is a platform to make HR training management easy including interviews and skills 
        description: |
          I was responsible for a 4 person team of 3 Data engineers and 1 ML engineer at Skillup, an SaaS helping RH in the company to provide a better experience providing training for their employees and evaluating impact.<br/>
          The team had a broad scope covering 4 principle subjects<br/>
          -External data flux from customer with client import (user, booked training, …)  & export (training evaluation)<br/>
          -Scrapping & cleaning training website for our catalog<br/>
          -Provide internal metrics & dashboard so products team can gain insights<br/>
          -Use latest AI technologies (LLM) to implement new feature for customer<br/>
          My first task when starting here was to put in place a CI/CD using github action to structure coding practice with automated testing and linting.<br/>
          All the while implementing team's rituals (postmortem, retrospective, daily, ownership rotation) as well as mentoring & pairing to share best coding practice (ruff, pydantic, pytest, …).<br/>
          This allowed  us to refactor existing code more easily and made the team more resilient with people being able to work on other team member code.<br/>
          The scrapping project required migration of Airflow orchestrated data pipelines that use Zyte for scrapping online training websites and extracting data using NLP models.<br/>
          The frontend of Skillup being developed in Typescript we rely on GRPC API to interact with their system.<br/>
          New services developed to allow complex importation of customer data were an Fastapi REST API to query Neo4j, MongoDB and PostgreSQL databases to reconcile existing data.<br/>
          We then provided a website in intern developed in Dash for the product team in case a manual validation was required.<br/>
          For the dashboarding we wanted to give a lot of autonomy for no tech savvy user and deployed both Metabase and Superset.<br/>
          For the IA related project we needed to do some optimisation of OpenIA POC for generation of job's skills based on role title
      - layout: left
        title: Talend
        link: https:wwww.talend.com
        sub_title: Big data Engineer
        caption: 2020 - 2022
        quote: >
          Talend is an open source data integration platform. It provides various software and services for data integration, data management, enterprise application integration, data quality, cloud storage and Big Data.
        description: |
          I was the first data engineer of a growing team of data scientists (5 to 7 people) that worked exclusively on state of the art machine learning for customer and internal needs.<br/>
          We got a separate AWS account and a dedicated kubernetes namespace for our dev environment, as we were the first to do machine learning at Talend so a lot of new processes needed to be put in place.<br/>
          In this context the first months were used to document and set guidelines for coding, testing, packaging and deployment using Github action and Jenkins.<br/>
          We experimented with multiple platforms to deliver our solutions<br/>
          -Data pipeline on Databricks with Scala and Python (Pyspark & Koala) to feed ML prediction models<br/>
          -Deployment and optimisation of models to detect anomalies in customer’s jobs in python & Flask<br/>
          -Extraction using Talend studio job(Java) to dump postgresql database on s3 bucket as parquet<br/>
          -Extraction and computation on a Snowflake datawarehouse<br/>
          We finally settled on Sagemaker for most of our projects and my role included finops responsibilities<br/>
          -Managing AWS account resources using terraform (tagging, creation, deletion)<br/>
          -Define with team's lead budget alerts and reports by slack/email with related dashboards to provide context<br/>
          -Evaluation and optimisation regarding architecture of Sagemaker pipeline, model and endpoint on AWS<br/>
          Other responsibilities included making people more aware of security with presentation, training on secureflag platform and setting audit tools in the CI with veracode.
      - layout: left
        title: Jobteaser
        link: https://www.jobteaser.com/en/home
        jobs:
        sub_title: Big Data Engineer
        caption: 2017 - 2019
        quote: >
          JobTeaser is a French company that provides recruitment solutions to companies for the recruitment of young talents and a free career center software towards higher education institutions in Europe.
        description: | # this will include new lines to allow paragraphs
          I joined Jobteaser as the second data engineer of a 5 person team that provided data analysis and machine learning for the whole company.<br/>
          The company was moving a startup to a scaleup so a lot of projects were doing migration from the legacy system to the new system.<br/>
          -Migrate Elastic Beanstalk REST API on AWS to Kubernetes.<br/>
          -Rewrite/deploy job classification REST API in Flask.<br/>
          -Write/deploy ETL and REST API for job recommendation project.<br/>
          Main project beside that was deploying the backbone of our data stack with a Kafka that allowed us to  stream our production database (MySQL) to our Postgresql using Debezium and change data capture patterns.<br/>
          We then loaded it into our datawarehouse (Redshift) that give data analyst opportunity to do realtime dashboard of backend activity
      - layout: left
        title:  Starclay
        link: http://www.starclay.fr/
        sub_title: Big Data Engineer at Ipsen
        caption: 2016 - 2017
        quote: >
          Ipsen is a pharmaceutical company
        description: | # this will include new lines to allow paragraphs
          Alongside another data engineer we exploring multiple projects to resolve issue encountered by marketing and research teams sharing progress during spring (3 weeks) for Ipsen an pharmaceutical company<br/>
          The two most advanced projects are listed below and all of them were hosted on AWS ec2 using docker-compose.<br/>
          -Team's responsible for handling clinical trials of new drugs needed insight on previous research done by other companies in the whole world, so we did data mining on official reports for specific diseases, cross referenced the sources then displayed it on an interactive map using Folium, we ended up pushing some fix to folium.<br/>
          -Marketing wanted to get feedback regarding new medicines on social media so we targeted tweets related to either the disease or drug,  developed an algorithm to identify key opinion leaders (either medical professional or public personality) then used nlp tools to do sentiment analysis.<br/>
      - layout: left
        title:  Starclay
        link: http://www.starclay.fr/
        sub_title: Big Data Engineer at Engie
        caption: 2015 - 2016
        quote: >
          Engie is the main electricity provider in France
        description: | # this will include new lines to allow paragraphs
          At Engie I worked on 2 projects with them<br/>
          PAP:<br/>
          With another data engineer and an app team in order to provide a solution to traveling salesmen that need to plan their road, get customer information and sign contracts on their tablets.<br/>
          We had a huge focus on geolocation and we had to put in place<br/>
          -Mongodb to store customer's data (60 millions users)<br/>
          -REST API written in Java for the application team<br/>
          -Fix customer addresses using google maps data and apache Lucene<br/>
          This project was successfully deployed and is still in production.<br/>
          GDF 360:<br/>
          To help desk support of Engie to provide better service to customers we implemented a POC in 6 months that displayed all known previous interactions between a customer and the company (call, bills, contract, …) using a graph database (neo4j) on a temporal axis.<br/>
      - layout: left
        title:  HSBC
        link: http://www.hsbc.fr/
        sub_title: Back-end developer
        caption: 2013 - 2015
        quote: >
          HSBC is a major banking company 
        description: | # this will include new lines to allow paragraphs
          At HSBC I was doing maintenance of banking features for customer account in collaboration with remote chinese team
  - title: Education
    layout: list
    content:
      - layout: top-left
        title: University Of Cergy-Pontoise (UCP)
        caption: 2013 - 2015
        sub_title: Master's degree in Computer Science
        description: |
          Main focus was on Java, Hadoop and SQL databases.


# Footer
footer_show_references: true
references_title: References on request

# Build settings
#theme: modern-resume-theme (Use this is you are hosting your resume yourself)
remote_theme: sproogen/modern-resume-theme

sass:
  sass_dir: _sass
  style: compressed

plugins:
 - jekyll-seo-tag

exclude : [
  "Gemfile",
  "Gemfile.lock",
  "node_modules",
  "vendor/bundle/",
  "vendor/cache/",
  "vendor/gems/",
  "vendor/ruby/",
  "lib/",
  "scripts/",
  "docker-compose.yml",
  ]
