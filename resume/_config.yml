baseurl: ''
repository: knil-sama/cdemonchy
version: 2
name: Clement Demonchy
title: Senior data engineer and data architect freelance
github_username: knil-sama
linkedin_username: clement-demonchy-55327778
darkmode: false
about_profile_image: images/profile.jpg
about_content: |-
  I have more than 10 years of experience working as a data engineer mainly in **Python** and **AWS**, assuming devops, architect and lead roles when needed.<br/>
      I prefer working on missions where I can focus on workflows that bring value while making sure that the tradeoff (cost, reliability, performance) are understood and managed.<br/>
      I appreciate when I can interact with the infra and product team directly.<br/>
      I am most proficient at: **AWS**, **Python**, **PostgreSQL**, **Kubernetes** and **Databricks**
      I have experiences on **Azure**, **GCP**, **Snowflake** as well as **Scala**, **Neo4j** and **MongoDB**
      I like to dabble in **Rust** and **TLA**
content:
- title: Projects
  layout: list
  content:
  - title: Blog
    layout: top-middle
    link: https://cdemonchy.com/blog/
    link_text: Blog
    description: Technical articles I wrote
  - title: Boite postale
    layout: top-middle
    link: https://boitepostale.cdemonchy.com/
    link_text: Demo
    description: Fetch open data from french government using **Python** to deploy an interactive map in Folium on **AWS** with **Github** **Actions**
  - title: Rubbersearch
    layout: top-middle
    description: '**Rust** code that processes a local file to expose data using a REST **API**'
  - title: ITM
    layout: top-middle
    description: |-
      Shows how to run an airflow workflow  in **Python**, distibuted with Flower and Celery to process images, extracting metadata before storing it in a **MongoDB** database,<br/>
                  there are some failures that will randomly appear when running the workflow to show how airflow handles it
- title: Experiences
  layout: list
  content:
  - title: Doctolib
    layout: left
    link: https://doctolib.fr/
    sub_title: Data Engineer
    quote: Doctolib is an appointment solution for healthcare professional and patient
    caption: 2025 - 2025
    description: |-
      In a team responsible for exposition and anonymization of production tables<br/>
                  -Change Data Capture of relational database to **Iceberg** table using **Debezium**<br/>
                  -Create mirror tables with **Pyspark** job on **EMR** serverless<br/>
                  -Create and maintain dataset on google cloud using **Crossplane** and **Kopf**<br/>
                  -Install resources on **GKE** using **Argocd**
  - title: Be:mo (at TotalEnergies)
    layout: left
    link: https://www.be-mo.io/
    sub_title: Data Engineer
    quote: Be:mo provide a B2B solutions to help recharge electric vehicule (truck and car)
    caption: 2023 - 2025
    description: "In a team responsible for working with electric vehicle supply equipment (EVSE) and exposing \n            it with REST **API** for multiple BtB companies<br/>\n            -Migration of data workflow from **Azure** synapse to **AWS** **Databricks**<br/>\n            -Writing CI job using **Github** **Actions** and **Databricks** bundle<br/>\n            -Using unity catalog to provide external access to dataset for services like PowerBi<br/>\n            -Usage of DLT table for streaming dataset<br/>\n            -Setting test for spark on dev machines to improve feedback<br/>\n            -Managing **AWS** and **Databricks** resources using **Terraform** (tagging, creation, deletion)<br/>\n            -Define alerts based on **Lambda** metrics using **Cloudwatch**<br/>\n            -Expose metrics of data workflow with **PostgreSQL** **Aurora** and **Grafana**<br/>\n            -Documentation on workflow using Mermaid on **Github**<br/>\n            -Analyzing and reducing cost on **Azure** and **AWS**, for example by  dividing cost by 5 for **AWS** **Lambda**     \n            exploiting caching between service and improving batching of SQS elements<br/>\n            -Optimize geolocalization operation on **PostgreSQL** and detect invalid coordinates"
  - title: Skillup
    layout: left
    link: https://skillup.co/
    sub_title: Lead data Engineer
    quote: Skillup is a platform to make HR training management easy including interviews and skills
    caption: 2022 - 2023
    description: |-
      I was responsible for a 4 person team of 3 Data engineers and 1 ML engineer at Skillup, an SaaS helping RH in the company to provide a better experience providing training for their employees and evaluating impact.<br/>
                  The team had a broad scope covering 4 principle subjects<br/>
                  -External data flux from customer with client import (user, booked training, …) and export (training evaluation)<br/>
                  -Scrapping and cleaning training website for our catalog<br/>
                  -Provide internal metrics and dashboard so products team can gain insights<br/>
                  -Use latest AI technologies (LLM) to implement new feature for customer<br/>
                  My first task when starting here was to put in place a CI/CD using **Github** **Actions** to structure coding practice with automated testing and linting.<br/>
                  All the while implementing team's rituals (postmortem, retrospective, daily, ownership rotation) as well as mentoring and pairing to share best coding practice (**Ruff**, **Pydantic**, **Pytest**, …).<br/>
                  This allowed  us to refactor existing code more easily and made the team more resilient with people being able to work on other team member code.<br/>
                  The scrapping project required migration of **Airflow** orchestrated data pipelines that use Zyte for scrapping online training websites and extracting data using NLP models.<br/>
                  The frontend of Skillup being developed in Typescript we rely on GRPC **API** to interact with their system.<br/>
                  New services developed to allow complex importation of customer data were an Fastapi REST **API** to query **Neo4j**, **MongoDB** and **PostgreSQL** databases to reconcile existing data.<br/>
                  We then provided a website in intern developed in Dash for the product team in case a manual validation was required.<br/>
                  For the dashboarding we wanted to give a lot of autonomy for no tech savvy user and deployed both Metabase and Superset.<br/>
                  For the IA related project we needed to do some optimisation of OpenIA POC for generation of job's skills based on role title
  - title: Talend
    layout: left
    link: https://talend.com/
    sub_title: Data Engineer
    quote: Talend is an open source data integration platform. It provides various software and services for data integration, data management, enterprise application integration, data quality, cloud storage and Big Data.
    caption: 2020 - 2022
    description: |-
      I was the first data engineer of a growing team of data scientists (5 to 7 people) that worked exclusively on state of the art machine learning for customer and internal needs.<br/>
                  We got a separate **AWS** account and a dedicated kubernetes namespace for our dev environment, as we were the first to do machine learning at Talend so a lot of new processes needed to be put in place.<br/>
                  In this context the first months were used to document and set guidelines for coding, testing, packaging and deployment using **Github** **Actions** and Jenkins.<br/>
                  We experimented with multiple platforms to deliver our solutions<br/>
                  -Data pipeline on **Databricks** with **Scala** and **Python** (**Pyspark** and Koala) to feed ML prediction models<br/>
                  -Deployment and optimisation of models to detect anomalies in customer’s jobs in **Python** and **Flask**<br/>
                  -Extraction using Talend studio job(Java) to dump **PostgreSQL** database on **S3** bucket as parquet<br/>
                  -Extraction and computation on a **Snowflake** datawarehouse<br/>
                  We finally settled on Sagemaker for most of our projects and my role included finops responsibilities<br/>
                  -Managing **AWS** account resources using terraform (tagging, creation, deletion)<br/>
                  -Define with team's lead budget alerts and reports by slack/email with related dashboards to provide context<br/>
                  -Evaluation and optimisation regarding architecture of Sagemaker pipeline, model and endpoint on **AWS**<br/>
                  Other responsibilities included making people more aware of security with presentation, training on secureflag platform and setting audit tools in the CI with veracode.
  - title: Jobteaser
    layout: left
    link: https://jobteaser.com/
    sub_title: Data Engineer
    quote: JobTeaser is a French company that provides recruitment solutions to companies for the recruitment of young talents and a free career center software towards higher education institutions in Europe.
    caption: 2017 - 2019
    description: |-
      I joined Jobteaser as the second data engineer of a 5 person team that provided data analysis and machine learning for the whole company.<br/>
                  The company was moving a startup to a scaleup so a lot of projects were doing migration from the legacy system to the new system.<br/>
                  -Migrate Elastic Beanstalk REST **API** on **AWS** to **Kubernetes**.<br/>
                  -Rewrite/deploy job classification REST **API** in **Flask**.<br/>
                  -Write/deploy ETL and REST **API** for job recommendation project.<br/>
                  Main project beside that was deploying the backbone of our data stack with a **Kafka** that allowed us to  stream our production database (MySQL) to our Postgresql using **Debezium** and change data capture patterns.<br/>
                  We then loaded it into our datawarehouse (**Redshift**) that give data analyst opportunity to do realtime dashboard of backend activity
  - title: Ipsen
    layout: left
    link: https://ipsen.com/
    sub_title: Data Engineer
    quote: Ipsen is a pharmaceutical company
    caption: 2016 - 2017
    description: |-
      Alongside another data engineer we exploring multiple projects to resolve issue encountered by marketing and research teams sharing progress during spring (3 weeks) for Ipsen an pharmaceutical company<br/>
                  The two most advanced projects are listed below and all of them were hosted on **AWS** **EC2** using docker-compose.<br/>
                  -Team's responsible for handling clinical trials of new drugs needed insight on previous research done by other companies in the whole world, so we did data mining on official reports for specific diseases, cross referenced the sources then displayed it on an interactive map using Folium, we ended up pushing some fix to folium.<br/>
                  -Marketing wanted to get feedback regarding new medicines on social media so we targeted tweets related to either the disease or drug,  developed an algorithm to identify key opinion leaders (either medical professional or public personality) then used nlp tools to do sentiment analysis.<br/>
  - title: Engie
    layout: left
    link: https://engie.com/
    sub_title: Data Engineer
    quote: Engie is a French multinational electric utility company
    caption: 2015 - 2016
    description: |-
      At Engie I worked on 2 projects with them<br/>
                  PAP:<br/>
                  With another data engineer and an app team in order to provide a solution to traveling salesmen that need to plan their road, get customer information and sign contracts on their tablets.<br/>
                  We had a huge focus on geolocation and we had to put in place<br/>
                  -Mongodb to store customer's data (60 millions users)<br/>
                  -REST **API** written in Java for the application team<br/>
                  -Fix customer addresses using google maps data and apache Lucene<br/>
                  This project was successfully deployed and is still in production.<br/>
                  GDF 360:<br/>
                  To help desk support of Engie to provide better service to customers we implemented a POC in 6 months that displayed all known previous interactions between a customer and the company (call, bills, contract, …) using a graph database (**Neo4j**) on a temporal axis.<br/>
- title: Education
  layout: list
  content:
  - title: University Of Cergy-Pontoise (UCP)
    layout: top-left
    sub_title: Master's degree in Computer Science
    caption: 2013 - 2015
    description: Main focus was on Java, Hadoop and SQL databases.
footer_show_references: true
references_title: References on request
remote_theme: sproogen/modern-resume-theme
sass:
  sass_dir: _sass
  style: compressed
plugins:
- jekyll-seo-tag
exclude:
- Gemfile
- Gemfile.lock
- node_modules
- vendor/bundle/
- vendor/cache/
- vendor/gems/
- vendor/ruby/
- lib/
- scripts/
- docker-compose.yml
